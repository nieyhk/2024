{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MtsJKvy1Xu2a"},"outputs":[],"source":["from google.colab.patches import cv2_imshow\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4453,"status":"ok","timestamp":1723719169235,"user":{"displayName":"김혜인","userId":"12045114964762593097"},"user_tz":-540},"id":"f7HLxF8kFjfs"},"outputs":[],"source":["import os\n","import torch\n","from PIL import Image\n","from pycocotools.coco import COCO\n","from torch.utils.data import Dataset\n","\n","\n","class COCODataset(Dataset):\n","    def __init__(self, root, train, transform=None):\n","        super().__init__()\n","        directory = \"train\" if train else \"val\"\n","        annotations = os.path.join(root, \"annotations\", f\"{directory}_annotations.json\")\n","\n","        self.coco = COCO(annotations)\n","        self.iamge_path = os.path.join(root, directory)\n","        self.transform = transform\n","\n","        self.categories = self._get_categories()\n","        self.data = self._load_data()\n","\n","    def _get_categories(self):\n","        categories = {0: \"background\"}\n","        for category in self.coco.cats.values():\n","            categories[category[\"id\"]] = category[\"name\"]\n","        return categories\n","\n","    def _load_data(self):\n","        data = []\n","        for _id in self.coco.imgs:\n","            file_name = self.coco.loadImgs(_id)[0][\"file_name\"]\n","            image_path = os.path.join(self.iamge_path, file_name)\n","            image = Image.open(image_path).convert(\"RGB\")\n","\n","            boxes = []\n","            labels = []\n","            anns = self.coco.loadAnns(self.coco.getAnnIds(_id))\n","            for ann in anns:\n","                x, y, w, h = ann[\"bbox\"]\n","\n","                boxes.append([x, y, x + w, y + h])\n","                labels.append(ann[\"category_id\"])\n","\n","            target = {\n","            \"image_id\": torch.LongTensor([_id]),\n","                \"boxes\": torch.FloatTensor(boxes),\n","                \"labels\": torch.LongTensor(labels)\n","            }\n","            data.append([image, target])\n","        return data\n","\n","    def __getitem__(self, index):\n","        image, target = self.data[index]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lsUzJhfKF1LV","outputId":"2d896953-3167-4760-84ce-dca271058cb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=1.03s)\n","creating index...\n","index created!\n"]}],"source":["from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","\n","def collator(batch):\n","    return tuple(zip(*batch))\n","\n","transform = transforms.Compose(\n","    [\n","        transforms.PILToTensor(),\n","        transforms.ConvertImageDtype(dtype=torch.float)\n","    ]\n",")\n","\n","train_dataset = COCODataset(\"/content/gdrive/My Drive/datasets/coco\", train=True, transform=transform)\n","test_dataset = COCODataset(\"/content/gdrive/My Drive/datasets/coco\", train=False, transform=transform)\n","\n","train_dataloader = DataLoader(\n","    train_dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=collator\n",")\n","test_dataloader = DataLoader(\n","    test_dataset, batch_size=1, shuffle=True, drop_last=True, collate_fn=collator\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CO0ZQxomF-9h"},"outputs":[],"source":["from torchvision import models\n","from torchvision import ops\n","from torchvision.models.detection import rpn\n","from torchvision.models.detection import FasterRCNN\n","\n","\n","backbone = models.vgg16(weights=\"VGG16_Weights.IMAGENET1K_V1\").features\n","backbone.out_channels = 512\n","\n","anchor_generator = rpn.AnchorGenerator(\n","    sizes=((32, 64, 128, 256, 512),),\n","    aspect_ratios=((0.5, 1.0, 2.0),)\n",")\n","roi_pooler = ops.MultiScaleRoIAlign(\n","    featmap_names=[\"0\"],\n","    output_size=(7, 7),\n","    sampling_ratio=2\n",")\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = FasterRCNN(\n","    backbone=backbone,\n","    num_classes=3,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",").to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSewYgbsGEl3"},"outputs":[],"source":["from torch import optim\n","\n","\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n","lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQZ6DVrYGGwj"},"outputs":[],"source":["for epoch in range(5):\n","    cost = 0.0\n","    for idx, (images, targets) in enumerate(train_dataloader):\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        cost += losses\n","\n","    lr_scheduler.step()\n","    cost = cost / len(train_dataloader)\n","    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1m-zOEHB7-G5eij3PrkqyeZUvqYPYRE-i"},"executionInfo":{"elapsed":122455,"status":"error","timestamp":1723715808707,"user":{"displayName":"김혜인","userId":"12045114964762593097"},"user_tz":-540},"id":"7C_-ikRWGMku","outputId":"3ab626d6-ff24-4067-e350-c5a12cf43371"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import numpy as np\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","from torchvision.transforms.functional import to_pil_image\n","\n","\n","def draw_bbox(ax, box, text, color):\n","    ax.add_patch(\n","        plt.Rectangle(\n","            xy=(box[0], box[1]),\n","            width=box[2] - box[0],\n","            height=box[3] - box[1],\n","            fill=False,\n","            edgecolor=color,\n","            linewidth=2,\n","        )\n","    )\n","    ax.annotate(\n","        text=text,\n","        xy=(box[0] - 5, box[1] - 5),\n","        color=color,\n","        weight=\"bold\",\n","        fontsize=13,\n","    )\n","\n","threshold = 0.5\n","categories = test_dataset.categories\n","with torch.no_grad():\n","    model.eval()\n","    for images, targets in test_dataloader:\n","        images = [image.to(device) for image in images]\n","        outputs = model(images)\n","\n","        boxes = outputs[0][\"boxes\"].to(\"cpu\").numpy()\n","        labels = outputs[0][\"labels\"].to(\"cpu\").numpy()\n","        scores = outputs[0][\"scores\"].to(\"cpu\").numpy()\n","\n","        boxes = boxes[scores >= threshold].astype(np.int32)\n","        labels = labels[scores >= threshold]\n","        scores = scores[scores >= threshold]\n","\n","        fig = plt.figure(figsize=(8, 8))\n","        ax = fig.add_subplot(1, 1, 1)\n","        plt.imshow(to_pil_image(images[0]))\n","\n","        for box, label, score in zip(boxes, labels, scores):\n","            draw_bbox(ax, box, f\"{categories[label]} - {score:.4f}\", \"red\")\n","\n","        tboxes = targets[0][\"boxes\"].numpy()\n","        tlabels = targets[0][\"labels\"].numpy()\n","        for box, label in zip(tboxes, tlabels):\n","            draw_bbox(ax, box, f\"{categories[label]}\", \"blue\")\n","\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YmZaFvDSGRCz","outputId":"6016f968-e992-4ef0-9d53-f3124ad47d43"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading and preparing results...\n","Converting ndarray to lists...\n","(18100, 7)\n","0/18100\n","DONE (t=0.45s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=0.68s).\n","Accumulating evaluation results...\n","DONE (t=0.25s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.002\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.021\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.035\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.019\n"]}],"source":["import numpy as np\n","from pycocotools.cocoeval import COCOeval\n","\n","\n","with torch.no_grad():\n","    model.eval()\n","    coco_detections = []\n","    for images, targets in test_dataloader:\n","        images = [img.to(device) for img in images]\n","        outputs = model(images)\n","\n","        for i in range(len(targets)):\n","            image_id = targets[i][\"image_id\"].data.cpu().numpy().tolist()[0]\n","            boxes = outputs[i][\"boxes\"].data.cpu().numpy()\n","            boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n","            boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n","            scores = outputs[i][\"scores\"].data.cpu().numpy()\n","            labels = outputs[i][\"labels\"].data.cpu().numpy()\n","\n","            for instance_id in range(len(boxes)):\n","                box = boxes[instance_id, :].tolist()\n","                prediction = np.array(\n","                    [\n","                        image_id,\n","                        box[0],\n","                        box[1],\n","                        box[2],\n","                        box[3],\n","                        float(scores[instance_id]),\n","                        int(labels[instance_id]),\n","                    ]\n","                )\n","                coco_detections.append(prediction)\n","\n","    coco_detections = np.asarray(coco_detections)\n","    coco_gt = test_dataloader.dataset.coco\n","    coco_dt = coco_gt.loadRes(coco_detections)\n","    coco_evaluator = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n","    coco_evaluator.evaluate()\n","    coco_evaluator.accumulate()\n","    coco_evaluator.summarize()"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPD3F+RKr+BqLjXRC6UFJbB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}